{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition – EDA & Mini Pipeline\n",
    "\n",
    "This notebook shows a small end-to-end workflow for **Emotion Recognition from Speech**:\n",
    "\n",
    "1. Load an example audio file from a dataset (e.g. RAVDESS / TESS / EMO-DB)\n",
    "2. Visualize waveform and spectrogram\n",
    "3. Extract MFCC features\n",
    "4. Load precomputed MFCC dataset (`.npz`) created by `extract_features.py`\n",
    "5. Build and train a small CNN\n",
    "6. Evaluate with a confusion matrix\n",
    "7. Run a sample prediction\n",
    "\n",
    "> **Note:** You must first download a dataset and run `src/extract_features.py` to create the `.npz` file used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load one example audio file\n",
    "\n",
    "Update `example_wav_path` below to point to a real `.wav` file from your dataset.\n",
    "For RAVDESS it may look like: `data/RAVDESS/Actor_01/03-01-03-01-01-01-01.wav`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change this to a real .wav in your data folder\n",
    "example_wav_path = \"data/RAVDESS/example.wav\"  # <- replace with real file path\n",
    "\n",
    "if not os.path.exists(example_wav_path):\n",
    "    print(\"WARNING: example_wav_path does not exist yet. Update the path above.\")\n",
    "else:\n",
    "    y, sr = librosa.load(example_wav_path, sr=22050, mono=True)\n",
    "    print(f\"Sample rate: {sr}, duration: {len(y)/sr:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize waveform and spectrogram\n",
    "\n",
    "Waveform shows amplitude over time, spectrogram shows frequency content (energy at each frequency vs time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(example_wav_path):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "\n",
    "    # Spectrogram\n",
    "    D = np.abs(librosa.stft(y))\n",
    "    DB = librosa.amplitude_to_db(D, ref=np.max)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(DB, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram (log-frequency)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping plots because example_wav_path is not set correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract MFCC features from a single file\n",
    "\n",
    "MFCCs (Mel-Frequency Cepstral Coefficients) are widely used audio features for speech and emotion recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_single(y, sr=22050, n_mfcc=40, max_len=174):\n",
    "    y_trim, _ = librosa.effects.trim(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y_trim, sr=sr, n_mfcc=n_mfcc)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    feat = np.vstack([mfcc, delta, delta2])  # shape: (n_mfcc*3, t)\n",
    "    if feat.shape[1] < max_len:\n",
    "        pad_width = max_len - feat.shape[1]\n",
    "        feat = np.pad(feat, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        feat = feat[:, :max_len]\n",
    "    return feat\n",
    "\n",
    "if os.path.exists(example_wav_path):\n",
    "    feat = extract_mfcc_single(y, sr=sr)\n",
    "    print(\"MFCC feature shape (channels x time):\", feat.shape)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(feat[:40], x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC (first 40 coefficients)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping MFCC extraction plot because example_wav_path is not set correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load precomputed MFCC dataset (.npz)\n",
    "\n",
    "Run this command **before** using the cell below:\n",
    "\n",
    "```bash\n",
    "python src/extract_features.py --data-dir data/RAVDESS --out-file data/ravdess_mfcc.npz\n",
    "```\n",
    "\n",
    "Then set `features_path` to that `.npz` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"data/ravdess_mfcc.npz\"  # change if needed\n",
    "\n",
    "if not os.path.exists(features_path):\n",
    "    print(\"WARNING: features_path does not exist yet. Run extract_features.py first.\")\n",
    "else:\n",
    "    data = np.load(features_path, allow_pickle=True)\n",
    "    X = data['X'].astype('float32')  # (N, channels, time)\n",
    "    y = data['y']                     # label indices\n",
    "    labels = list(data['labels'])\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"Labels:\", labels)\n",
    "\n",
    "    # Normalize per sample\n",
    "    X = (X - X.mean(axis=(1, 2), keepdims=True)) / (X.std(axis=(1, 2), keepdims=True) + 1e-6)\n",
    "\n",
    "    # Train / validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a small CNN model\n",
    "\n",
    "We treat the MFCC feature map as a 2D image (channels × time) with a single depth channel. The model is similar to what `src/train.py` uses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, n_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Reshape((*input_shape, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "if os.path.exists(features_path):\n",
    "    input_shape = X_train.shape[1:]\n",
    "    n_classes = len(labels)\n",
    "    model = build_cnn(input_shape, n_classes)\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"Skipping model creation because features_path is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the CNN (demo training)\n",
    "\n",
    "This is a small demo training loop. For serious training, you can increase epochs or use `src/train.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None\n",
    "\n",
    "if os.path.exists(features_path):\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping training because features_path is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    hist = history.history\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(hist['loss'], label='train')\n",
    "    plt.plot(hist['val_loss'], label='val')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(hist['accuracy'], label='train')\n",
    "    plt.plot(hist['val_accuracy'], label='val')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: confusion matrix & classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(features_path) and history is not None:\n",
    "    y_val_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "    print(classification_report(y_val, y_val_pred, target_names=labels))\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Validation confusion matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping evaluation because training did not run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Single-sample prediction example\n",
    "\n",
    "We reuse the MFCC extraction and model to predict emotion for one new audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update this path to another audio sample\n",
    "predict_wav_path = example_wav_path\n",
    "\n",
    "if os.path.exists(predict_wav_path) and history is not None:\n",
    "    y_pred_audio, sr_pred = librosa.load(predict_wav_path, sr=22050, mono=True)\n",
    "    feat_pred = extract_mfcc_single(y_pred_audio, sr=sr_pred)\n",
    "    # normalize like others (sample-wise)\n",
    "    feat_pred = (feat_pred - feat_pred.mean()) / (feat_pred.std() + 1e-6)\n",
    "    inp = np.expand_dims(feat_pred, axis=0)\n",
    "    probs = model.predict(inp)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    print(\"Predicted emotion:\", labels[idx])\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"  {label}: {probs[i]:.3f}\")\n",
    "else:\n",
    "    print(\"Skipping prediction example. Check paths and that training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Next steps / ideas\n",
    "- Try different model architectures: LSTM, CRNN, or deeper CNNs.\n",
    "- Add data augmentation (noise, time stretch, pitch shift).\n",
    "- Use `KerasTuner` for hyperparameter tuning.\n",
    "- Export the trained model and build a small demo app (Streamlit / Gradio) for interactive testing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
